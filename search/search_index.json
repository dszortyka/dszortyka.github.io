{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hey there, welcome! Welcome to my GitHub Pages. What to expect here: Technical content posts Technical documentation How To guides Past projects You can contact me via Linkedin","title":"Home"},{"location":"#hey-there-welcome","text":"Welcome to my GitHub Pages. What to expect here: Technical content posts Technical documentation How To guides Past projects You can contact me via Linkedin","title":"Hey there, welcome!"},{"location":"Tech%20Blog/basic-dns-setup-on-debian/","text":"Basic DNS Setup on Debian using Bind There are situations when we want to run a local POC/Demo at our local environment and a DNS server is required. Most of times, a simple change on /etc/hosts file is enough to simulate a network, but other cases will really require you to have a DNS running. There are many options to handle a DNS server, even running it from a docker container. In my case, I have a \"utils\" Virtual Machine where common services are performed from this host, as well a DNS Server . I also wanted the correct name for a local dns server, then I found RFC8375 which contains exactly the information I was looking for. So, my local network name is called home.arpa and my local router is setup with a /22 network class, IP range from 192.168.12.1 to 192.168.15.255. In this way, I can have some kind of playground for my local tests. This post will contains the modifications required in order to have this \"setup\" in a local Virtual Machine. Basic Info OS : Debian 11 DNS Server : Bind (a.k.a. Named) Local Zone : home.arpa Network : /22 - 192.168.12.1 to 192.168.15.255 DNS Server IP : 192.168.15.205 Setup The very first step is to install Debian 11 in your virtualization app. Install basic OS Package and Bind9 Bind9 is the DNS Server we'll be using. apt install sudo net-tools mlocate bind9 bind9utils vim Setup Static IP Below setup will disable IPv6. vi /etc/network/interfaces # This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). source /etc/network/interfaces.d/* # The loopback network interface auto lo iface lo inet loopback # The primary network interface #allow-hotplug enp0s3 #iface enp0s3 inet dhcp auto enp0s3 iface enp0s3 inet static address 192.168.15.205 netmask 255.255.252.0 gateway 192.168.15.1 dns-domain home.arpa dns-nameservers 192.168.15.205 # This is an autoconfigured IPv6 interface # iface enp0s3 inet6 auto Restart the network service and re-connect to the server. systemctl restart networking Adjust /etc/resolv.conf In my case, I wanted the \"DNS\" Server to first search itself and then, if not found, go ahead and search on google dns (8.8.8.8). cat /etc/resolv.conf nameserver 192.168.15.205 nameserver 8.8.8.8 Create your DNS Local Zone (home.arpa) To do so, few steps are required since the original folders doesn't exist by default. mkdir -p /var/lib/bind chown root:bind /var/lib/bind* touch home.arpa.db # this will contain the entries for domain home.arpa touch 15.168.192.home.arpa.db # this will contain the reverse DNS entries for home.arpa Once ready, go ahead and add your entries: cat /var/lib/bind/home.arpa.db $TTL 3600 @ IN SOA dns.home.arpa. root.home.arpa. ( 1 ; serial 3600 ; refresh 1h 600 ; retry 10min 86400 ; expire 1day 600 ; negative cache ttl 1h ); @ IN NS dns.home.arpa. dns IN A 192.168.15.205 k8master IN A 192.168.15.210 k8node1 IN A 192.168.15.211 k8node2 IN A 192.168.15.212 Reverse dns zone: cat 15.168.192.home.arpa.db @ IN SOA dns.home.arpa. root.home.arpa. ( 1 ; serial 3600 ; refresh 1h 600 ; retry 10min 86400 ; expire 1day 600 ; negative cache ttl 1h ); @ IN NS dns.home.arpa. 205 IN PTR dns.home.arpa. 210 IN PTR k8master.home.arpa. 211 IN PTR k8node1.home.arpa. 212 IN PTR k8node2.home.arpa. Modify the named.conf files In debian, the named.conf file comes inside /etc/bind/ folder. named.conf named.conf.local named.conf.log named.conf.options zones.rfc1918 named.conf include \"/etc/bind/named.conf.options\"; include \"/etc/bind/named.conf.log\"; include \"/etc/bind/named.conf.local\"; include \"/etc/bind/named.conf.default-zones\"; named.conf.local // // Do any local configuration here // // Consider adding the 1918 zones here, if they are not used in your // organization //include \"/etc/bind/zones.rfc1918\"; zone \"home.arpa\" in { type master; file \"/var/lib/bind/home.arpa.db\"; }; zone \"15.168.192.in-addr.arpa\" in { type master; file \"/var/lib/bind/15.168.192.home.arpa.db\"; }; named.conf.options // // Do any local configuration here // // Consider adding the 1918 zones here, if they are not used in your // organization //include \"/etc/bind/zones.rfc1918\"; zone \"home.arpa\" in { type master; file \"/var/lib/bind/home.arpa.db\"; }; zone \"15.168.192.in-addr.arpa\" in { type master; file \"/var/lib/bind/15.168.192.home.arpa.db\"; }; root@dns:/etc/bind# cat named.conf.options options { directory \"/var/cache/bind\"; // If there is a firewall between you and nameservers you want // to talk to, you may need to fix the firewall to allow multiple // ports to talk. See http://www.kb.cert.org/vuls/id/800113 // If your ISP provided one or more IP addresses for stable // nameservers, you probably want to use them as forwarders. // Uncomment the following block, and insert the addresses replacing // the all-0's placeholder. // forwarders { // 0.0.0.0; // }; //======================================================================== // If BIND logs error messages about the root key being expired, // you will need to update your keys. See https://www.isc.org/bind-keys //======================================================================== dnssec-validation auto; listen-on { 127.0.0.1; 192.168.15.205; }; listen-on-v6 { none; }; allow-recursion { any; }; version none; }; named.conf.log This is an additional file in order to record logs from our dns server. It will be required to create folders , files and adjust their permissions in the filesystem as well in the apparmor to avoid lack of privileges and security. # create log folder and files mkdir -p /var/log/bind touch /var/log/bind.log touch /var/log/security_info.log touch /var/log/update_debug.log chown root:bind /var/log/bind/* Finally, create named.conf.log logging { channel update_debug { file \"/var/log/bind/update_debug.log\" versions 3 size 100k; severity debug; print-severity yes; print-time yes; }; channel security_info { file \"/var/log/bind/security_info.log\" versions 1 size 100k; severity info; print-severity yes; print-time yes; }; channel bind_log { file \"/var/log/bind/bind.log\" versions 3 size 1m; severity info; print-category yes; print-severity yes; print-time yes; }; category default { bind_log; }; category lame-servers { null; }; category update { update_debug; }; category update-security { update_debug; }; category security { security_info; }; }; zones.rfc1918 In this file, we'll need to comment out the entry related to the network 192.168.* . If that's not your network range, then probably doesn't need to change anything. //zone \"168.192.in-addr.arpa\" { type master; file \"/etc/bind/db.empty\"; }; Start services Once everything is setup, time to start the services and check for logs. systemctl restart named systemctl status named If any errors occur, you can monitor those errors by running: journalctl |grep named You can validate if your DNS Server is resolving properly: nslookup k8master.home.arpa nslookup 192.168.15.210","title":"How to setup a Basic DNS Server with Debian and Bind"},{"location":"Tech%20Blog/basic-dns-setup-on-debian/#basic-dns-setup-on-debian-using-bind","text":"There are situations when we want to run a local POC/Demo at our local environment and a DNS server is required. Most of times, a simple change on /etc/hosts file is enough to simulate a network, but other cases will really require you to have a DNS running. There are many options to handle a DNS server, even running it from a docker container. In my case, I have a \"utils\" Virtual Machine where common services are performed from this host, as well a DNS Server . I also wanted the correct name for a local dns server, then I found RFC8375 which contains exactly the information I was looking for. So, my local network name is called home.arpa and my local router is setup with a /22 network class, IP range from 192.168.12.1 to 192.168.15.255. In this way, I can have some kind of playground for my local tests. This post will contains the modifications required in order to have this \"setup\" in a local Virtual Machine.","title":"Basic DNS Setup on Debian using Bind"},{"location":"Tech%20Blog/basic-dns-setup-on-debian/#basic-info","text":"OS : Debian 11 DNS Server : Bind (a.k.a. Named) Local Zone : home.arpa Network : /22 - 192.168.12.1 to 192.168.15.255 DNS Server IP : 192.168.15.205","title":"Basic Info"},{"location":"Tech%20Blog/basic-dns-setup-on-debian/#setup","text":"The very first step is to install Debian 11 in your virtualization app.","title":"Setup"},{"location":"Tech%20Blog/basic-dns-setup-on-debian/#install-basic-os-package-and-bind9","text":"Bind9 is the DNS Server we'll be using. apt install sudo net-tools mlocate bind9 bind9utils vim","title":"Install basic OS Package and Bind9"},{"location":"Tech%20Blog/basic-dns-setup-on-debian/#setup-static-ip","text":"Below setup will disable IPv6. vi /etc/network/interfaces # This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). source /etc/network/interfaces.d/* # The loopback network interface auto lo iface lo inet loopback # The primary network interface #allow-hotplug enp0s3 #iface enp0s3 inet dhcp auto enp0s3 iface enp0s3 inet static address 192.168.15.205 netmask 255.255.252.0 gateway 192.168.15.1 dns-domain home.arpa dns-nameservers 192.168.15.205 # This is an autoconfigured IPv6 interface # iface enp0s3 inet6 auto Restart the network service and re-connect to the server. systemctl restart networking","title":"Setup Static IP"},{"location":"Tech%20Blog/basic-dns-setup-on-debian/#adjust-etcresolvconf","text":"In my case, I wanted the \"DNS\" Server to first search itself and then, if not found, go ahead and search on google dns (8.8.8.8). cat /etc/resolv.conf nameserver 192.168.15.205 nameserver 8.8.8.8","title":"Adjust /etc/resolv.conf"},{"location":"Tech%20Blog/basic-dns-setup-on-debian/#create-your-dns-local-zone-homearpa","text":"To do so, few steps are required since the original folders doesn't exist by default. mkdir -p /var/lib/bind chown root:bind /var/lib/bind* touch home.arpa.db # this will contain the entries for domain home.arpa touch 15.168.192.home.arpa.db # this will contain the reverse DNS entries for home.arpa Once ready, go ahead and add your entries: cat /var/lib/bind/home.arpa.db $TTL 3600 @ IN SOA dns.home.arpa. root.home.arpa. ( 1 ; serial 3600 ; refresh 1h 600 ; retry 10min 86400 ; expire 1day 600 ; negative cache ttl 1h ); @ IN NS dns.home.arpa. dns IN A 192.168.15.205 k8master IN A 192.168.15.210 k8node1 IN A 192.168.15.211 k8node2 IN A 192.168.15.212 Reverse dns zone: cat 15.168.192.home.arpa.db @ IN SOA dns.home.arpa. root.home.arpa. ( 1 ; serial 3600 ; refresh 1h 600 ; retry 10min 86400 ; expire 1day 600 ; negative cache ttl 1h ); @ IN NS dns.home.arpa. 205 IN PTR dns.home.arpa. 210 IN PTR k8master.home.arpa. 211 IN PTR k8node1.home.arpa. 212 IN PTR k8node2.home.arpa.","title":"Create your DNS Local Zone (home.arpa)"},{"location":"Tech%20Blog/basic-dns-setup-on-debian/#modify-the-namedconf-files","text":"In debian, the named.conf file comes inside /etc/bind/ folder. named.conf named.conf.local named.conf.log named.conf.options zones.rfc1918 named.conf include \"/etc/bind/named.conf.options\"; include \"/etc/bind/named.conf.log\"; include \"/etc/bind/named.conf.local\"; include \"/etc/bind/named.conf.default-zones\"; named.conf.local // // Do any local configuration here // // Consider adding the 1918 zones here, if they are not used in your // organization //include \"/etc/bind/zones.rfc1918\"; zone \"home.arpa\" in { type master; file \"/var/lib/bind/home.arpa.db\"; }; zone \"15.168.192.in-addr.arpa\" in { type master; file \"/var/lib/bind/15.168.192.home.arpa.db\"; }; named.conf.options // // Do any local configuration here // // Consider adding the 1918 zones here, if they are not used in your // organization //include \"/etc/bind/zones.rfc1918\"; zone \"home.arpa\" in { type master; file \"/var/lib/bind/home.arpa.db\"; }; zone \"15.168.192.in-addr.arpa\" in { type master; file \"/var/lib/bind/15.168.192.home.arpa.db\"; }; root@dns:/etc/bind# cat named.conf.options options { directory \"/var/cache/bind\"; // If there is a firewall between you and nameservers you want // to talk to, you may need to fix the firewall to allow multiple // ports to talk. See http://www.kb.cert.org/vuls/id/800113 // If your ISP provided one or more IP addresses for stable // nameservers, you probably want to use them as forwarders. // Uncomment the following block, and insert the addresses replacing // the all-0's placeholder. // forwarders { // 0.0.0.0; // }; //======================================================================== // If BIND logs error messages about the root key being expired, // you will need to update your keys. See https://www.isc.org/bind-keys //======================================================================== dnssec-validation auto; listen-on { 127.0.0.1; 192.168.15.205; }; listen-on-v6 { none; }; allow-recursion { any; }; version none; }; named.conf.log This is an additional file in order to record logs from our dns server. It will be required to create folders , files and adjust their permissions in the filesystem as well in the apparmor to avoid lack of privileges and security. # create log folder and files mkdir -p /var/log/bind touch /var/log/bind.log touch /var/log/security_info.log touch /var/log/update_debug.log chown root:bind /var/log/bind/* Finally, create named.conf.log logging { channel update_debug { file \"/var/log/bind/update_debug.log\" versions 3 size 100k; severity debug; print-severity yes; print-time yes; }; channel security_info { file \"/var/log/bind/security_info.log\" versions 1 size 100k; severity info; print-severity yes; print-time yes; }; channel bind_log { file \"/var/log/bind/bind.log\" versions 3 size 1m; severity info; print-category yes; print-severity yes; print-time yes; }; category default { bind_log; }; category lame-servers { null; }; category update { update_debug; }; category update-security { update_debug; }; category security { security_info; }; }; zones.rfc1918 In this file, we'll need to comment out the entry related to the network 192.168.* . If that's not your network range, then probably doesn't need to change anything. //zone \"168.192.in-addr.arpa\" { type master; file \"/etc/bind/db.empty\"; };","title":"Modify the named.conf files"},{"location":"Tech%20Blog/basic-dns-setup-on-debian/#start-services","text":"Once everything is setup, time to start the services and check for logs. systemctl restart named systemctl status named If any errors occur, you can monitor those errors by running: journalctl |grep named You can validate if your DNS Server is resolving properly: nslookup k8master.home.arpa nslookup 192.168.15.210","title":"Start services"},{"location":"Tech%20Blog/getting-started-github-pages/","text":"Getting started with GitHub Pages Nowadays, most of companies tend to use private git repositories. This means that most of our work keeps restrict to internal public. That's fine, that's ok. The idea of having a personnal GitHub Page is to provide comments on IT Topics, tools and the challenges faced every week. As part of this process, I created this GitHub page and had to start with the basics: Setup a GitHub Page . Setting up GitHub Pages The instructions are pretty straight forward... The problem? My understanding from Mkdocs is that the source content will be located in the Main branch and inside the /docs folder. As we can see from the image below, the text says: Source: Your GitHub Pages site is currently built from the <name> branch My understand is that this option should be set to \"Main\" branch and /docs folder. But it is NOT! gh-pages branch and / (root) folder needs to be selected. Reference links below provide more information. Reference Links GitHub Pages GitHub MkDocs Deploy Action Mkdocs Material Material for Mkdocs https://github.com/mkdocs/mkdocs/issues/2255#issuecomment-922165394 https://github.com/mkdocs/mkdocs/discussions/2569","title":"How to setup GitHub Pages"},{"location":"Tech%20Blog/getting-started-github-pages/#getting-started-with-github-pages","text":"Nowadays, most of companies tend to use private git repositories. This means that most of our work keeps restrict to internal public. That's fine, that's ok. The idea of having a personnal GitHub Page is to provide comments on IT Topics, tools and the challenges faced every week. As part of this process, I created this GitHub page and had to start with the basics: Setup a GitHub Page .","title":"Getting started with GitHub Pages"},{"location":"Tech%20Blog/getting-started-github-pages/#setting-up-github-pages","text":"The instructions are pretty straight forward... The problem? My understanding from Mkdocs is that the source content will be located in the Main branch and inside the /docs folder. As we can see from the image below, the text says: Source: Your GitHub Pages site is currently built from the <name> branch My understand is that this option should be set to \"Main\" branch and /docs folder. But it is NOT! gh-pages branch and / (root) folder needs to be selected. Reference links below provide more information.","title":"Setting up GitHub Pages"},{"location":"Tech%20Blog/getting-started-github-pages/#reference-links","text":"GitHub Pages GitHub MkDocs Deploy Action Mkdocs Material Material for Mkdocs https://github.com/mkdocs/mkdocs/issues/2255#issuecomment-922165394 https://github.com/mkdocs/mkdocs/discussions/2569","title":"Reference Links"},{"location":"Tech%20Blog/k8s-prometheus-grafana-redis-monitoring/","text":"Monitoring Redis with Prometheus and Grafana I'm having the opportunity to work more and more with kubernetes at home and at my job. Recently, I was challenged to deploy Prometheus+Grafana stack for a Redis application. All of them running on top of a kubernetes cluster. Or like my manager says, I'm going thru my \"Kubernetes Dojo\". As per google definition: It is an honored place where students and masters come together for deliberate practice to develop their > > skills. In organizations, a Dojo is where teams learn complex skills, practices, and tools that help them > develop better products faster. Dojos use immersive learning. Besides that, I also liked the idea of having someone caring for my learning and bringing me challenges that are aligned on what I want to learn and the company needs. The Dojo approach is more principle-driven than process-driven. It is a methodology that grows and > evolves based on experience and experimentation. There are a few principles common to most Dojos: core > tenets that define the Dojo as a concept. Prioritize learning over delivery. Align learning to the team\u2019s actual work. Focus on safety, not on failure. Work small and exploit feedback. Make everything visible. Use the buddy system. Success depends on commitment. Use Case Image you have the following cenario: A Redis cluster that is used from application to subscribe and publish messages into a some kind of Queue. Multiple applications will be reading and writing data to the Redis Cluster. At some point, you need to scrape your Redis Cluster metrics to your Prometheus+Grafana stack. All of these applications are currently packed all together and can be easily installed Helm charts . However, new users would prefer to learn how to create those manifests manually and deploy them into a Kubernetes cluster. Important This post and github repository are used for learning purposes and probably not following yet the best practices. The whole starting point and reference is the original creator of rq-exporter tool. At the original github repository, you'll find a docker-compose.yml file. That file was my starting point using with kompose convert to generate kubernetes manifests. From that point, I started my changes. Python RQ Prometheus Exporter tool: https://github.com/mdawar/rq-exporter Pierre Mdawar repo: https://github.com/mdawar If you just want a excelent starting point with a good cenario for testing, I'd recommend go with the Docker Compose instructions from Pierre's page. K8s Manifests The whole idea was learn how to write kubernetes manifests. From there, I knew the Kompose tool, which uses a docker-compose.yml file to generate k8s manifests. It's not perfect, but a very good starting point. From there, just google the missing parts or whatever you want to modify or add. Applications Prometheus For example, the kompose convert only generate the Persistent Volume Claims, I was trying to convert it to ConfigMap properties to make easier to recreate the PODs. Grafana I'm still working to convert Grafana configuration files to a ConfigMap setup, for now it's still using the PVCs created by kompose convert . Redis Redis is a standalong execution and it's using standard 123456 password. Soon I should improve the manifests with secrets, just lazy.. rq-exporter Exporter for Redis can be found here: https://github.com/mdawar/rq-exporter. As mentioned earlier, the whole idea of these manifest files are based on rq-exporter docker-compose file. rq-dashboard This tool allows visualization of simple metrics generated by Redis server. It's based on Dockerhub image eoranged/rq-dashboard:legacy . rq-enqueue and rq-worker From rq-exporter docker compose example, it starts Enqueue and Worker pods to generate data into the Redis Cluster. They keep running on background. Important to mention here that I generated a docker image (from original Dockerfile) and included the remaining python files. In that way, my image dszortyka/rq-exporter:latest contains all the python files needed for the example cenario. You can find the manifests resulted from this exercise on: https://github.com/dszortyka/k8s-redis-exporter-graf-prom cd k8s/ kubectl apply -f redis/k8s \\ -f rq-exporter/k8s \\ -f grafana/k8s \\ -f prometheus/k8s \\ -f rq-enqueue/k8s \\ -f rq-worker/k8s \\ -f rq-dashboard/k8s ~/DEV/k8s-redis-exporter-graf-prom/k8s main \u276f kubectl apply -f redis/k8s \\ -f rq-exporter/k8s \\ -f grafana/k8s \\ -f prometheus/k8s \\ -f rq-enqueue/k8s \\ -f rq-worker/k8s \\ -f rq-dashboard/k8s namespace/redis created deployment.apps/redis created service/redis created namespace/rq-exporter created deployment.apps/rq-exporter created service/rq-exporter created namespace/grafana created configmap/grafana-server-conf created persistentvolumeclaim/grafana-claim0 created persistentvolumeclaim/grafana-claim1 created persistentvolumeclaim/grafana-claim2 created persistentvolumeclaim/grafana-data created deployment.apps/grafana created service/grafana created namespace/prometheus created configmap/prometheus-server-conf created service/prometheus created deployment.apps/prometheus created persistentvolumeclaim/prometheus-claim0 created namespace/enqueue created persistentvolumeclaim/enqueue-claim0 created deployment.apps/enqueue created namespace/rq-worker created deployment.apps/rq-worker created namespace/rq-dashboard created deployment.apps/rq-dashboard created service/rq-dashboard created tree k8s/ k8s \u251c\u2500\u2500 grafana \u2502 \u2514\u2500\u2500 k8s \u2502 \u251c\u2500\u2500 000-grafana-namespace.yaml \u2502 \u251c\u2500\u2500 010-grafana-configmap.yaml \u2502 \u251c\u2500\u2500 files \u2502 \u2502 \u251c\u2500\u2500 grafana-dashboards.yml \u2502 \u2502 \u251c\u2500\u2500 grafana-datasources.yml \u2502 \u2502 \u2514\u2500\u2500 rq-dashboard.json \u2502 \u251c\u2500\u2500 grafana-claim0-persistentvolumeclaim.yaml \u2502 \u251c\u2500\u2500 grafana-claim1-persistentvolumeclaim.yaml \u2502 \u251c\u2500\u2500 grafana-claim2-persistentvolumeclaim.yaml \u2502 \u251c\u2500\u2500 grafana-data-persistentvolumeclaim.yaml \u2502 \u251c\u2500\u2500 grafana-deployment.yaml \u2502 \u2514\u2500\u2500 grafana-service.yaml \u251c\u2500\u2500 prometheus \u2502 \u2514\u2500\u2500 k8s \u2502 \u251c\u2500\u2500 000-prometheus-namespace.yaml \u2502 \u251c\u2500\u2500 010-prometheus-configmap.yaml \u2502 \u251c\u2500\u2500 020-prometheus-service.yaml \u2502 \u251c\u2500\u2500 040-prometheus-deployment.yaml \u2502 \u2514\u2500\u2500 prometheus-claim0-persistentvolumeclaim.yaml \u251c\u2500\u2500 redis \u2502 \u2514\u2500\u2500 k8s \u2502 \u251c\u2500\u2500 000-redis-namespace.yaml \u2502 \u251c\u2500\u2500 redis-deployment.yaml \u2502 \u2514\u2500\u2500 redis-service.yaml \u251c\u2500\u2500 rq-dashboard \u2502 \u2514\u2500\u2500 k8s \u2502 \u251c\u2500\u2500 000-rq-dashboard-namespace.yaml \u2502 \u251c\u2500\u2500 rq-dashboard-deployment.yaml \u2502 \u2514\u2500\u2500 rq-dashboard-service.yaml \u251c\u2500\u2500 rq-enqueue \u2502 \u2514\u2500\u2500 k8s \u2502 \u251c\u2500\u2500 000-enqueue-namespace.yaml \u2502 \u251c\u2500\u2500 010-enqueue-pvc-claim0.yaml \u2502 \u2514\u2500\u2500 enqueue-deployment.yaml \u251c\u2500\u2500 rq-exporter \u2502 \u251c\u2500\u2500 docker-compose.yml \u2502 \u251c\u2500\u2500 image \u2502 \u2502 \u251c\u2500\u2500 Dockerfile \u2502 \u2502 \u251c\u2500\u2500 docker-compose \u2502 \u2502 \u2502 \u2514\u2500\u2500 project \u2502 \u2502 \u2502 \u251c\u2500\u2500 custom.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 enqueue.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 jobs.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 queues.py \u2502 \u2502 \u251c\u2500\u2500 requirements.txt \u2502 \u2502 \u251c\u2500\u2500 rq_exporter \u2502 \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 __main__.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 __version__.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 collector.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 config.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 exporter.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 utils.py \u2502 \u2502 \u2514\u2500\u2500 setup.py \u2502 \u2514\u2500\u2500 k8s \u2502 \u251c\u2500\u2500 000-rq-exporter-namespace.yaml \u2502 \u251c\u2500\u2500 rq-exporter-deployment.yaml \u2502 \u2514\u2500\u2500 rq-exporter-service.yaml \u2514\u2500\u2500 rq-worker \u2514\u2500\u2500 k8s \u251c\u2500\u2500 000-rq-worker-namespace.yaml \u2514\u2500\u2500 rq-worker-deployment.yaml NAMESPACE NAME READY STATUS RESTARTS AGE default pod/busybox 1/1 Running 90 (16m ago) 3d18h enqueue pod/enqueue-84658d757b-2mqkt 1/1 Running 3 (31s ago) 61s grafana pod/grafana-54fc684bbc-pqpwp 1/1 Running 0 62s kube-system pod/calico-kube-controllers-c44b4545-chf7s 1/1 Running 2 (2d17h ago) 3d18h kube-system pod/calico-node-vhnfl 1/1 Running 4 (2d17h ago) 3d18h kube-system pod/coredns-6d4b75cb6d-r8m2k 1/1 Running 2 (2d17h ago) 3d18h kube-system pod/etcd-minikube 1/1 Running 3 (2d17h ago) 3d18h kube-system pod/kube-apiserver-minikube 1/1 Running 4 (2d17h ago) 3d18h kube-system pod/kube-controller-manager-minikube 1/1 Running 4 (2d17h ago) 3d18h kube-system pod/kube-proxy-svfbt 1/1 Running 3 (2d17h ago) 3d18h kube-system pod/kube-scheduler-minikube 1/1 Running 3 (2d17h ago) 3d18h kube-system pod/storage-provisioner 1/1 Running 17 (11h ago) 3d18h prometheus pod/prometheus-597b4967c-9vlvk 1/1 Running 0 62s redis pod/redis-75c4f49968-75824 1/1 Running 0 63s rq-dashboard pod/rq-dashboard-f95dcdbf-q9lvk 1/1 Running 0 61s rq-exporter pod/rq-exporter-5b9cf74dc4-s6kb2 1/1 Running 3 (29s ago) 62s rq-worker pod/rq-worker-56d8cbb45f-5l59b 1/1 Running 2 (40s ago) 61s rq-worker pod/rq-worker-56d8cbb45f-vrxcv 1/1 Running 2 (42s ago) 61s NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE bitbucket service/bitbucket NodePort 10.105.94.217 <none> 7990:30990/TCP,7999:30999/TCP 3d17h bitbucket service/postgresql ClusterIP None <none> 5432/TCP 3d18h default service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 3d18h grafana service/grafana ClusterIP 10.102.22.56 <none> 3000/TCP 62s kube-system service/kube-dns ClusterIP 10.96.0.10 <none> 53/UDP,53/TCP,9153/TCP 3d18h prometheus service/prometheus ClusterIP 10.97.158.165 <none> 9090/TCP 62s redis service/redis ClusterIP None <none> 6379/TCP 63s rq-dashboard service/rq-dashboard ClusterIP 10.104.21.144 <none> 9181/TCP 61s rq-exporter service/rq-exporter ClusterIP 10.101.203.160 <none> 9726/TCP 62s NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system daemonset.apps/calico-node 1 1 1 1 1 kubernetes.io/os=linux 3d18h kube-system daemonset.apps/kube-proxy 1 1 1 1 1 kubernetes.io/os=linux 3d18h NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE enqueue deployment.apps/enqueue 1/1 1 1 61s grafana deployment.apps/grafana 1/1 1 1 62s kube-system deployment.apps/calico-kube-controllers 1/1 1 1 3d18h kube-system deployment.apps/coredns 1/1 1 1 3d18h prometheus deployment.apps/prometheus 1/1 1 1 62s redis deployment.apps/redis 1/1 1 1 63s rq-dashboard deployment.apps/rq-dashboard 1/1 1 1 61s rq-exporter deployment.apps/rq-exporter 1/1 1 1 63s rq-worker deployment.apps/rq-worker 2/2 2 2 61s NAMESPACE NAME DESIRED CURRENT READY AGE enqueue replicaset.apps/enqueue-84658d757b 1 1 1 61s grafana replicaset.apps/grafana-54fc684bbc 1 1 1 62s kube-system replicaset.apps/calico-kube-controllers-c44b4545 1 1 1 3d18h kube-system replicaset.apps/coredns-6d4b75cb6d 1 1 1 3d18h prometheus replicaset.apps/prometheus-597b4967c 1 1 1 62s redis replicaset.apps/redis-75c4f49968 1 1 1 63s rq-dashboard replicaset.apps/rq-dashboard-f95dcdbf 1 1 1 61s rq-exporter replicaset.apps/rq-exporter-5b9cf74dc4 1 1 1 62s rq-worker replicaset.apps/rq-worker-56d8cbb45f 2 2 2 61s","title":"Kubernetes - Redis monitoring with Prometheus and Grafana"},{"location":"Tech%20Blog/k8s-prometheus-grafana-redis-monitoring/#monitoring-redis-with-prometheus-and-grafana","text":"I'm having the opportunity to work more and more with kubernetes at home and at my job. Recently, I was challenged to deploy Prometheus+Grafana stack for a Redis application. All of them running on top of a kubernetes cluster. Or like my manager says, I'm going thru my \"Kubernetes Dojo\". As per google definition: It is an honored place where students and masters come together for deliberate practice to develop their > > skills. In organizations, a Dojo is where teams learn complex skills, practices, and tools that help them > develop better products faster. Dojos use immersive learning. Besides that, I also liked the idea of having someone caring for my learning and bringing me challenges that are aligned on what I want to learn and the company needs. The Dojo approach is more principle-driven than process-driven. It is a methodology that grows and > evolves based on experience and experimentation. There are a few principles common to most Dojos: core > tenets that define the Dojo as a concept. Prioritize learning over delivery. Align learning to the team\u2019s actual work. Focus on safety, not on failure. Work small and exploit feedback. Make everything visible. Use the buddy system. Success depends on commitment.","title":"Monitoring Redis with Prometheus and Grafana"},{"location":"Tech%20Blog/k8s-prometheus-grafana-redis-monitoring/#use-case","text":"Image you have the following cenario: A Redis cluster that is used from application to subscribe and publish messages into a some kind of Queue. Multiple applications will be reading and writing data to the Redis Cluster. At some point, you need to scrape your Redis Cluster metrics to your Prometheus+Grafana stack. All of these applications are currently packed all together and can be easily installed Helm charts . However, new users would prefer to learn how to create those manifests manually and deploy them into a Kubernetes cluster.","title":"Use Case"},{"location":"Tech%20Blog/k8s-prometheus-grafana-redis-monitoring/#important","text":"This post and github repository are used for learning purposes and probably not following yet the best practices. The whole starting point and reference is the original creator of rq-exporter tool. At the original github repository, you'll find a docker-compose.yml file. That file was my starting point using with kompose convert to generate kubernetes manifests. From that point, I started my changes. Python RQ Prometheus Exporter tool: https://github.com/mdawar/rq-exporter Pierre Mdawar repo: https://github.com/mdawar If you just want a excelent starting point with a good cenario for testing, I'd recommend go with the Docker Compose instructions from Pierre's page.","title":"Important"},{"location":"Tech%20Blog/k8s-prometheus-grafana-redis-monitoring/#k8s-manifests","text":"The whole idea was learn how to write kubernetes manifests. From there, I knew the Kompose tool, which uses a docker-compose.yml file to generate k8s manifests. It's not perfect, but a very good starting point. From there, just google the missing parts or whatever you want to modify or add. Applications Prometheus For example, the kompose convert only generate the Persistent Volume Claims, I was trying to convert it to ConfigMap properties to make easier to recreate the PODs. Grafana I'm still working to convert Grafana configuration files to a ConfigMap setup, for now it's still using the PVCs created by kompose convert . Redis Redis is a standalong execution and it's using standard 123456 password. Soon I should improve the manifests with secrets, just lazy.. rq-exporter Exporter for Redis can be found here: https://github.com/mdawar/rq-exporter. As mentioned earlier, the whole idea of these manifest files are based on rq-exporter docker-compose file. rq-dashboard This tool allows visualization of simple metrics generated by Redis server. It's based on Dockerhub image eoranged/rq-dashboard:legacy . rq-enqueue and rq-worker From rq-exporter docker compose example, it starts Enqueue and Worker pods to generate data into the Redis Cluster. They keep running on background. Important to mention here that I generated a docker image (from original Dockerfile) and included the remaining python files. In that way, my image dszortyka/rq-exporter:latest contains all the python files needed for the example cenario. You can find the manifests resulted from this exercise on: https://github.com/dszortyka/k8s-redis-exporter-graf-prom cd k8s/ kubectl apply -f redis/k8s \\ -f rq-exporter/k8s \\ -f grafana/k8s \\ -f prometheus/k8s \\ -f rq-enqueue/k8s \\ -f rq-worker/k8s \\ -f rq-dashboard/k8s ~/DEV/k8s-redis-exporter-graf-prom/k8s main \u276f kubectl apply -f redis/k8s \\ -f rq-exporter/k8s \\ -f grafana/k8s \\ -f prometheus/k8s \\ -f rq-enqueue/k8s \\ -f rq-worker/k8s \\ -f rq-dashboard/k8s namespace/redis created deployment.apps/redis created service/redis created namespace/rq-exporter created deployment.apps/rq-exporter created service/rq-exporter created namespace/grafana created configmap/grafana-server-conf created persistentvolumeclaim/grafana-claim0 created persistentvolumeclaim/grafana-claim1 created persistentvolumeclaim/grafana-claim2 created persistentvolumeclaim/grafana-data created deployment.apps/grafana created service/grafana created namespace/prometheus created configmap/prometheus-server-conf created service/prometheus created deployment.apps/prometheus created persistentvolumeclaim/prometheus-claim0 created namespace/enqueue created persistentvolumeclaim/enqueue-claim0 created deployment.apps/enqueue created namespace/rq-worker created deployment.apps/rq-worker created namespace/rq-dashboard created deployment.apps/rq-dashboard created service/rq-dashboard created tree k8s/ k8s \u251c\u2500\u2500 grafana \u2502 \u2514\u2500\u2500 k8s \u2502 \u251c\u2500\u2500 000-grafana-namespace.yaml \u2502 \u251c\u2500\u2500 010-grafana-configmap.yaml \u2502 \u251c\u2500\u2500 files \u2502 \u2502 \u251c\u2500\u2500 grafana-dashboards.yml \u2502 \u2502 \u251c\u2500\u2500 grafana-datasources.yml \u2502 \u2502 \u2514\u2500\u2500 rq-dashboard.json \u2502 \u251c\u2500\u2500 grafana-claim0-persistentvolumeclaim.yaml \u2502 \u251c\u2500\u2500 grafana-claim1-persistentvolumeclaim.yaml \u2502 \u251c\u2500\u2500 grafana-claim2-persistentvolumeclaim.yaml \u2502 \u251c\u2500\u2500 grafana-data-persistentvolumeclaim.yaml \u2502 \u251c\u2500\u2500 grafana-deployment.yaml \u2502 \u2514\u2500\u2500 grafana-service.yaml \u251c\u2500\u2500 prometheus \u2502 \u2514\u2500\u2500 k8s \u2502 \u251c\u2500\u2500 000-prometheus-namespace.yaml \u2502 \u251c\u2500\u2500 010-prometheus-configmap.yaml \u2502 \u251c\u2500\u2500 020-prometheus-service.yaml \u2502 \u251c\u2500\u2500 040-prometheus-deployment.yaml \u2502 \u2514\u2500\u2500 prometheus-claim0-persistentvolumeclaim.yaml \u251c\u2500\u2500 redis \u2502 \u2514\u2500\u2500 k8s \u2502 \u251c\u2500\u2500 000-redis-namespace.yaml \u2502 \u251c\u2500\u2500 redis-deployment.yaml \u2502 \u2514\u2500\u2500 redis-service.yaml \u251c\u2500\u2500 rq-dashboard \u2502 \u2514\u2500\u2500 k8s \u2502 \u251c\u2500\u2500 000-rq-dashboard-namespace.yaml \u2502 \u251c\u2500\u2500 rq-dashboard-deployment.yaml \u2502 \u2514\u2500\u2500 rq-dashboard-service.yaml \u251c\u2500\u2500 rq-enqueue \u2502 \u2514\u2500\u2500 k8s \u2502 \u251c\u2500\u2500 000-enqueue-namespace.yaml \u2502 \u251c\u2500\u2500 010-enqueue-pvc-claim0.yaml \u2502 \u2514\u2500\u2500 enqueue-deployment.yaml \u251c\u2500\u2500 rq-exporter \u2502 \u251c\u2500\u2500 docker-compose.yml \u2502 \u251c\u2500\u2500 image \u2502 \u2502 \u251c\u2500\u2500 Dockerfile \u2502 \u2502 \u251c\u2500\u2500 docker-compose \u2502 \u2502 \u2502 \u2514\u2500\u2500 project \u2502 \u2502 \u2502 \u251c\u2500\u2500 custom.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 enqueue.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 jobs.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 queues.py \u2502 \u2502 \u251c\u2500\u2500 requirements.txt \u2502 \u2502 \u251c\u2500\u2500 rq_exporter \u2502 \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 __main__.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 __version__.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 collector.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 config.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 exporter.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 utils.py \u2502 \u2502 \u2514\u2500\u2500 setup.py \u2502 \u2514\u2500\u2500 k8s \u2502 \u251c\u2500\u2500 000-rq-exporter-namespace.yaml \u2502 \u251c\u2500\u2500 rq-exporter-deployment.yaml \u2502 \u2514\u2500\u2500 rq-exporter-service.yaml \u2514\u2500\u2500 rq-worker \u2514\u2500\u2500 k8s \u251c\u2500\u2500 000-rq-worker-namespace.yaml \u2514\u2500\u2500 rq-worker-deployment.yaml NAMESPACE NAME READY STATUS RESTARTS AGE default pod/busybox 1/1 Running 90 (16m ago) 3d18h enqueue pod/enqueue-84658d757b-2mqkt 1/1 Running 3 (31s ago) 61s grafana pod/grafana-54fc684bbc-pqpwp 1/1 Running 0 62s kube-system pod/calico-kube-controllers-c44b4545-chf7s 1/1 Running 2 (2d17h ago) 3d18h kube-system pod/calico-node-vhnfl 1/1 Running 4 (2d17h ago) 3d18h kube-system pod/coredns-6d4b75cb6d-r8m2k 1/1 Running 2 (2d17h ago) 3d18h kube-system pod/etcd-minikube 1/1 Running 3 (2d17h ago) 3d18h kube-system pod/kube-apiserver-minikube 1/1 Running 4 (2d17h ago) 3d18h kube-system pod/kube-controller-manager-minikube 1/1 Running 4 (2d17h ago) 3d18h kube-system pod/kube-proxy-svfbt 1/1 Running 3 (2d17h ago) 3d18h kube-system pod/kube-scheduler-minikube 1/1 Running 3 (2d17h ago) 3d18h kube-system pod/storage-provisioner 1/1 Running 17 (11h ago) 3d18h prometheus pod/prometheus-597b4967c-9vlvk 1/1 Running 0 62s redis pod/redis-75c4f49968-75824 1/1 Running 0 63s rq-dashboard pod/rq-dashboard-f95dcdbf-q9lvk 1/1 Running 0 61s rq-exporter pod/rq-exporter-5b9cf74dc4-s6kb2 1/1 Running 3 (29s ago) 62s rq-worker pod/rq-worker-56d8cbb45f-5l59b 1/1 Running 2 (40s ago) 61s rq-worker pod/rq-worker-56d8cbb45f-vrxcv 1/1 Running 2 (42s ago) 61s NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE bitbucket service/bitbucket NodePort 10.105.94.217 <none> 7990:30990/TCP,7999:30999/TCP 3d17h bitbucket service/postgresql ClusterIP None <none> 5432/TCP 3d18h default service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 3d18h grafana service/grafana ClusterIP 10.102.22.56 <none> 3000/TCP 62s kube-system service/kube-dns ClusterIP 10.96.0.10 <none> 53/UDP,53/TCP,9153/TCP 3d18h prometheus service/prometheus ClusterIP 10.97.158.165 <none> 9090/TCP 62s redis service/redis ClusterIP None <none> 6379/TCP 63s rq-dashboard service/rq-dashboard ClusterIP 10.104.21.144 <none> 9181/TCP 61s rq-exporter service/rq-exporter ClusterIP 10.101.203.160 <none> 9726/TCP 62s NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system daemonset.apps/calico-node 1 1 1 1 1 kubernetes.io/os=linux 3d18h kube-system daemonset.apps/kube-proxy 1 1 1 1 1 kubernetes.io/os=linux 3d18h NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE enqueue deployment.apps/enqueue 1/1 1 1 61s grafana deployment.apps/grafana 1/1 1 1 62s kube-system deployment.apps/calico-kube-controllers 1/1 1 1 3d18h kube-system deployment.apps/coredns 1/1 1 1 3d18h prometheus deployment.apps/prometheus 1/1 1 1 62s redis deployment.apps/redis 1/1 1 1 63s rq-dashboard deployment.apps/rq-dashboard 1/1 1 1 61s rq-exporter deployment.apps/rq-exporter 1/1 1 1 63s rq-worker deployment.apps/rq-worker 2/2 2 2 61s NAMESPACE NAME DESIRED CURRENT READY AGE enqueue replicaset.apps/enqueue-84658d757b 1 1 1 61s grafana replicaset.apps/grafana-54fc684bbc 1 1 1 62s kube-system replicaset.apps/calico-kube-controllers-c44b4545 1 1 1 3d18h kube-system replicaset.apps/coredns-6d4b75cb6d 1 1 1 3d18h prometheus replicaset.apps/prometheus-597b4967c 1 1 1 62s redis replicaset.apps/redis-75c4f49968 1 1 1 63s rq-dashboard replicaset.apps/rq-dashboard-f95dcdbf 1 1 1 61s rq-exporter replicaset.apps/rq-exporter-5b9cf74dc4 1 1 1 62s rq-worker replicaset.apps/rq-worker-56d8cbb45f 2 2 2 61s","title":"K8s Manifests"},{"location":"Tech%20Blog/python-unit-tests-compilation/","text":"Python Unit Tests Compilation I've been stu dying && struggling to have a clear and easier understanding of how Unit Tests works. I mean, the concept of Mocking, \"faking\" values and output results is a bit trick specially when you don't have a history with Development teams. Sometimes the tricky part is to identify what needs to be tested, and then how to Mock the data and make assertions against it. Below is a compilation of links for Unit Tests, Mocks and other information I found after hundreds of tabs open. Links Why your mock doesn't work A very detailed ilustration how objects and attributes behaves when inside modules. How the modules are imported in your code and how you mock them in your tests can make a huge difference. Introduction: Getting Started with Mocking in Python An introduction to Mock in Python Detailed How To's Culture of Unit Testing An approach on how to implement a culture of testing inside any IT area, from development to simple automation scripts. Having targets and metrics to move forward with software deployment is important not only for the business but also for the IT folks. How Mock Can Improve Your Unit Tests: Part 1 Basic Class implementation detailing the steps to mock a module, a class and its attributes. Also, an important where and when Mock objects. How Mock Can Improve Your Unit Tests: Part 2 Moving forward with the previous link and examples, Caktus Group also provided a second part implementation for Unit Tests, showing how to control object behavior. Python Unittest Examples: Mocking and Patching Provides a range of examples for several test operations. Gists Mock Methods and Examples Mocking Cheat Sheet Mocking in Python Cheat Sheet Of Python Mock Slides with many examples","title":"Python Unit Tests Compilation"},{"location":"Tech%20Blog/python-unit-tests-compilation/#python-unit-tests-compilation","text":"I've been stu dying && struggling to have a clear and easier understanding of how Unit Tests works. I mean, the concept of Mocking, \"faking\" values and output results is a bit trick specially when you don't have a history with Development teams. Sometimes the tricky part is to identify what needs to be tested, and then how to Mock the data and make assertions against it. Below is a compilation of links for Unit Tests, Mocks and other information I found after hundreds of tabs open.","title":"Python Unit Tests Compilation"},{"location":"Tech%20Blog/python-unit-tests-compilation/#links","text":"Why your mock doesn't work A very detailed ilustration how objects and attributes behaves when inside modules. How the modules are imported in your code and how you mock them in your tests can make a huge difference.","title":"Links"},{"location":"Tech%20Blog/python-unit-tests-compilation/#introduction","text":"Getting Started with Mocking in Python An introduction to Mock in Python","title":"Introduction:"},{"location":"Tech%20Blog/python-unit-tests-compilation/#detailed-how-tos","text":"Culture of Unit Testing An approach on how to implement a culture of testing inside any IT area, from development to simple automation scripts. Having targets and metrics to move forward with software deployment is important not only for the business but also for the IT folks. How Mock Can Improve Your Unit Tests: Part 1 Basic Class implementation detailing the steps to mock a module, a class and its attributes. Also, an important where and when Mock objects. How Mock Can Improve Your Unit Tests: Part 2 Moving forward with the previous link and examples, Caktus Group also provided a second part implementation for Unit Tests, showing how to control object behavior. Python Unittest Examples: Mocking and Patching Provides a range of examples for several test operations.","title":"Detailed How To's"},{"location":"Tech%20Blog/python-unit-tests-compilation/#gists","text":"Mock Methods and Examples","title":"Gists"},{"location":"Tech%20Blog/python-unit-tests-compilation/#mocking-cheat-sheet","text":"Mocking in Python Cheat Sheet Of Python Mock Slides with many examples","title":"Mocking Cheat Sheet"}]}